"""
CrewAI Flow that:
1. Summarises recent chat.
2. Lets an LLM break the user prompt into subâ€‘tasks (structured output).
3. Lets the same LLM map each subâ€‘task to the best agent (structured output).
4. Runs subâ€‘crews sequentially, feeding context from previous subtasks into subsequent ones.
5. Synthesises the final answer.
"""

from __future__ import annotations

import logging
from asyncio import TimeoutError, wait_for
from typing import Any, Dict, List, Optional

from config import LLM_DELEGATOR, get_llm_for_request
from crewai import LLM, Crew, Process, Task
from crewai.flow.flow import Flow, listen, start
from services.secrets import get_secret

from .actions.handler import extract_final_answer_actions
from .helpers.context_optimization import optimize_context_block
from .helpers.context_utils import create_focused_task_description, summarize_previous_outputs, truncate_text
from .helpers.retry_utils import retry_with_backoff
from .helpers.utils import parse_llm_structured_output
from .orchestration_state import (
    Assignment,
    AssignmentPlan,
    OrchestrationState,
    ProcessingTime,
    SubtaskOutput,
    SubtaskPlan,
    Telemetry,
    TokenUsage,
)
from .progress_listener import (
    emit_final_answer_actions,
    emit_final_complete,
    emit_flow_end,
    emit_flow_start,
    emit_subtask_dispatch,
    emit_subtask_result,
    emit_synthesis_complete,
    emit_synthesis_start,
)
from .registry.agent_registry import AgentRegistry

logger = logging.getLogger(__name__)

# Constants to control execution flow
MAX_SUBTASKS = 5  # Keep original limit for complex tasks
SUBTASK_TIMEOUT = 180  # Maximum time (seconds) to wait for a subtask to complete
DEFAULT_TASK_OUTPUT = "Unable to complete this task within the allowed constraints."


# --------------------------------------------------------------------- #
# Flow implementation
# --------------------------------------------------------------------- #
class OrchestrationFlow(Flow[OrchestrationState]):
    def __init__(
        self,
        standard_model: str = "gemini/gemini-2.5-flash-preview-04-17",
        request_id: Optional[str] = None,
        user_selected_model: Optional[str] = None,
    ):
        super().__init__()
        self.request_id = request_id
        self.standard_model = standard_model
        self.standard_model_api_key = get_secret("GeminiApiKey")
        self.user_selected_model = user_selected_model

        # Use a more efficient model for simple tasks
        self.efficient_model = "gemini/gemini-1.5-flash"

        # Set up LLM instance based on user selection
        self.llm_instance = get_llm_for_request(user_selected_model)

    # 1ï¸âƒ£  Summarise recent chat -----------------------------------------
    @start()
    async def initialise(self) -> None:
        print(f"Initialising flow with state: {self.state}")

        # Emit flow start event if streaming
        if self.request_id:
            await emit_flow_start(self.request_id)

        chat_prompt = self.state.chat_prompt
        chat_history = self.state.chat_history
        self.state.chat_prompt = chat_prompt

        # Only summarize chat history that's relevant to the current prompt
        history_text = "\n".join(chat_history)
        if history_text:
            # Always prioritize user-selected LLM (MOR), fallback to efficient model only if no user selection
            if self.user_selected_model:
                logger.info(f"ðŸ”¥ ORCHESTRATION - Using MOR LLM for chat summarization: {self.user_selected_model}")
                llm = self.llm_instance
            else:
                logger.info(f"âš ï¸ ORCHESTRATION - Falling back to Gemini for chat summarization: {self.efficient_model}")
                llm = LLM(model=self.efficient_model, api_key=self.standard_model_api_key)

            @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
            def summarize_chat():
                return llm.call(
                    "Review the conversation history and extract ONLY information that is relevant "
                    "to answering this current prompt/request. If no information is relevant, return "
                    "an empty string. Do not include irrelevant context.\n\n"
                    f"Current prompt: {chat_prompt}\n\n"
                    f"Conversation history:\n{history_text}"
                )

            try:
                resp = summarize_chat()
                self.state.chat_history_summary = resp.strip()
            except Exception as e:
                logger.error(f"Failed to summarize chat history: {e}")
                self.state.chat_history_summary = ""  # Fallback to empty summary
        else:
            self.state.chat_history_summary = ""

    # 2ï¸âƒ£  Subâ€‘task planning (LLM functionâ€‘call, structured) -------------
    @listen(initialise)
    def create_subtasks(self):
        # Curate the full prompt, with
        prompt = (
            "Break this goal into minimal essential subtasks (1-3 preferred, max 4). "
            "At the end, we will synthesize the results from the subtasks into a final answer. "
            "So, don't include subtasks that are summarization or post-processing related to synthesis. "
            "Each subtask should be specific, actionable, and necessary. "
            "Avoid redundancy - each subtask should have a distinct purpose. "
            "The subtasks should directly address the core goal - do not be distracted by context. "
            "Return ONLY valid JSON.\n"
            f"Goal: {self.state.chat_prompt}"
        )

        # Only add chat summary if it's meaningful
        if self.state.chat_history_summary:
            prompt += (
                "\nNote: Below is a summary of relevant details from prior chat history. "
                "Use these details only if they provide specific information needed to complete subtasks. "
                "Do not create subtasks just to incorporate this context - focus on the core goal.\n"
                f"Relevant chat details: {self.state.chat_history_summary}"
            )

        # Use user-selected LLM if available, otherwise use standard model for subtask planning
        if self.user_selected_model:
            logger.info(f"ðŸ”¥ ORCHESTRATION - Using MOR LLM for subtask planning: {self.user_selected_model}")
            from config import create_morllm

            llm = create_morllm(model=self.user_selected_model, response_format=SubtaskPlan)
        else:
            logger.info(f"âš ï¸ ORCHESTRATION - Falling back to Gemini for subtask planning: {self.standard_model}")
            llm = LLM(model=self.standard_model, response_format=SubtaskPlan, api_key=self.standard_model_api_key)

        @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
        def create_subtask_plan():
            return llm.call(prompt)

        try:
            plan_response = create_subtask_plan()
            plan = parse_llm_structured_output(plan_response, SubtaskPlan, logger, "SubtaskPlan")

            # Limit number of subtasks as a precaution to prevent too much complexity,
            # sometimes the subtask plan forgets that we synthesize separately.
            if plan and plan.subtasks and len(plan.subtasks) > MAX_SUBTASKS:
                plan.subtasks = plan.subtasks[:MAX_SUBTASKS]

            self.state.subtasks = plan.subtasks if plan and plan.subtasks else []

            # If no subtasks were created, try Gemini fallback
            if not self.state.subtasks:
                logger.warning("MOR LLM failed to create subtasks, falling back to Gemini")
                fallback_llm = LLM(
                    model=self.standard_model, response_format=SubtaskPlan, api_key=self.standard_model_api_key
                )

                @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
                def create_subtask_plan_with_gemini():
                    return fallback_llm.call(prompt)

                try:
                    fallback_plan_response = create_subtask_plan_with_gemini()
                    fallback_plan = parse_llm_structured_output(
                        fallback_plan_response, SubtaskPlan, logger, "SubtaskPlan (Gemini fallback)"
                    )

                    if fallback_plan and fallback_plan.subtasks:
                        if len(fallback_plan.subtasks) > MAX_SUBTASKS:
                            fallback_plan.subtasks = fallback_plan.subtasks[:MAX_SUBTASKS]
                        self.state.subtasks = fallback_plan.subtasks
                    else:
                        self.state.subtasks = ["Complete the user's request"]
                except Exception as fallback_e:
                    logger.error(f"Gemini fallback also failed for subtask planning: {fallback_e}")
                    self.state.subtasks = ["Complete the user's request"]
        except Exception as e:
            logger.error(f"Failed to create subtask plan with MOR LLM: {e}")
            # Fallback to Gemini for subtask planning
            logger.warning("âš ï¸ ORCHESTRATION - MOR LLM failed for subtask planning, falling back to Gemini")
            try:
                fallback_llm = LLM(
                    model=self.standard_model, response_format=SubtaskPlan, api_key=self.standard_model_api_key
                )

                @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
                def create_subtask_plan_with_gemini_fallback():
                    return fallback_llm.call(prompt)

                fallback_plan_response = create_subtask_plan_with_gemini_fallback()
                fallback_plan = parse_llm_structured_output(
                    fallback_plan_response, SubtaskPlan, logger, "SubtaskPlan (Gemini fallback)"
                )

                if fallback_plan and fallback_plan.subtasks:
                    if len(fallback_plan.subtasks) > MAX_SUBTASKS:
                        fallback_plan.subtasks = fallback_plan.subtasks[:MAX_SUBTASKS]
                    self.state.subtasks = fallback_plan.subtasks
                else:
                    self.state.subtasks = ["Complete the user's request"]
            except Exception as fallback_e:
                logger.error(f"Gemini fallback also failed for subtask planning: {fallback_e}")
                # Final fallback to a single generic subtask
                self.state.subtasks = ["Complete the user's request"]

    # 3ï¸âƒ£  Agent assignment (LLM decides, structured) --------------------
    @listen(create_subtasks)
    def assign_agents(self):
        agent_descriptions = AgentRegistry.llm_choice_payload()
        prompt = (
            "Select 1-4 best agents per subtask. "
            "Match agent expertise to task requirements. "
            "Prefer specialized agents over generalists. "
            "Return ONLY valid JSON.\n"
            f"Agents:\n{agent_descriptions}\n"
            f"Subtasks: {self.state.subtasks}"
        )
        # Use user-selected LLM if available, otherwise use standard model for agent assignment
        if self.user_selected_model:
            logger.info(f"ðŸ”¥ ORCHESTRATION - Using MOR LLM for agent assignment: {self.user_selected_model}")
            from config import create_morllm

            llm = create_morllm(model=self.user_selected_model, response_format=AssignmentPlan)
        else:
            logger.info(f"âš ï¸ ORCHESTRATION - Falling back to Gemini for agent assignment: {self.standard_model}")
            llm = LLM(model=self.standard_model, response_format=AssignmentPlan, api_key=self.standard_model_api_key)

        @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
        def assign_agents_to_tasks():
            return llm.call(prompt)

        try:
            mapping_response = assign_agents_to_tasks()
            mapping = parse_llm_structured_output(mapping_response, AssignmentPlan, logger, "AssignmentPlan")

            if mapping and mapping.assignments:
                self.state.assignments = mapping.assignments
            else:
                # If MOR LLM failed to provide valid assignments, fallback to Gemini
                logger.warning("MOR LLM failed to provide valid assignments, falling back to Gemini")
                fallback_llm = LLM(
                    model=self.standard_model, response_format=AssignmentPlan, api_key=self.standard_model_api_key
                )

                @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
                def assign_agents_with_gemini():
                    return fallback_llm.call(prompt)

                try:
                    fallback_response = assign_agents_with_gemini()
                    fallback_mapping = parse_llm_structured_output(
                        fallback_response, AssignmentPlan, logger, "AssignmentPlan (Gemini fallback)"
                    )

                    if fallback_mapping and fallback_mapping.assignments:
                        self.state.assignments = fallback_mapping.assignments
                    else:
                        raise Exception("Both MOR and Gemini failed to provide valid assignments")
                except Exception as fallback_e:
                    logger.error(f"Gemini fallback also failed: {fallback_e}")
                    # Last resort: assign first available agent to each subtask
                    available_agents = AgentRegistry.all_names()
                    fallback_agent = available_agents[0] if available_agents else "general"
                    logger.warning(f"Using last resort fallback agent: {fallback_agent}")
                    self.state.assignments = [
                        Assignment(subtask=subtask, agents=[fallback_agent]) for subtask in self.state.subtasks
                    ]
        except Exception as e:
            logger.error(f"Failed to assign agents with MOR LLM: {e}")
            # Fallback to Gemini for agent assignment
            logger.warning("âš ï¸ ORCHESTRATION - MOR LLM failed for agent assignment, falling back to Gemini")
            try:
                fallback_llm = LLM(
                    model=self.standard_model, response_format=AssignmentPlan, api_key=self.standard_model_api_key
                )

                @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
                def assign_agents_with_gemini_fallback():
                    return fallback_llm.call(prompt)

                fallback_response = assign_agents_with_gemini_fallback()
                fallback_mapping = parse_llm_structured_output(
                    fallback_response, AssignmentPlan, logger, "AssignmentPlan (Gemini fallback)"
                )

                if fallback_mapping and fallback_mapping.assignments:
                    self.state.assignments = fallback_mapping.assignments
                else:
                    raise Exception("Gemini fallback also failed to provide valid assignments")
            except Exception as fallback_e:
                logger.error(f"Gemini fallback also failed: {fallback_e}")
                # Last resort: assign first available agent to each subtask
                available_agents = AgentRegistry.all_names()
                fallback_agent = available_agents[0] if available_agents else "general"
                logger.warning(f"Using last resort fallback agent: {fallback_agent}")
                self.state.assignments = [
                    Assignment(subtask=subtask, agents=[fallback_agent]) for subtask in self.state.subtasks
                ]

    # 4ï¸âƒ£  Run each subâ€‘task sequentially ----------------------------------
    @listen(assign_agents)
    async def run_sub_crews(self):
        import time

        async def _execute(
            subtask: str, agents: List[str], previous_outputs: Optional[List[SubtaskOutput]] = None
        ) -> SubtaskOutput:
            # Get only necessary agents for efficiency
            crew_agents = [AgentRegistry.get(agent_name) for agent_name in agents]

            # Early exit if no agents were found
            if not crew_agents:
                logger.error(f"No agents found for subtask: {subtask}")
                return SubtaskOutput(
                    subtask=subtask,
                    output="Error: No agents available to execute this task",
                    agents=agents,
                    telemetry=Telemetry(
                        processing_time=ProcessingTime(start_time=time.time(), end_time=time.time(), duration=0)
                    ),
                )

            # Create smart summary of previous work - more context for recent tasks
            previous_context = ""
            if previous_outputs:
                previous_context = (
                    summarize_previous_outputs(
                        previous_outputs,
                        max_total_context=10000,  # Increased for better flow
                    )
                    + "\n"
                )

            # Create focused task description with proper context
            enhanced_subtask = create_focused_task_description(
                subtask=subtask,
                chat_prompt=self.state.chat_prompt,
                chat_summary=self.state.chat_history_summary,
                previous_context=previous_context,
                max_total_length=1500,
            )

            # Track processing time
            start_time = time.time()

            # Keep using LLM_DELEGATOR for crew management (agents should use Gemini)
            # Only the high-level orchestration uses MOR LLM
            logger.info(f"ðŸ¤– AGENTS - Using Gemini delegator for crew management: {LLM_DELEGATOR}")
            manager_llm = LLM_DELEGATOR

            crew = Crew(
                agents=crew_agents,
                tasks=[
                    Task(
                        description=enhanced_subtask,
                        expected_output="Clear, complete answer to the specific task",
                        agent=crew_agents[0],
                    )
                ],
                process=Process.sequential,
                manager_llm=manager_llm,
                verbose=False,
            )

            try:
                # Add timeout to prevent hanging tasks
                result = await wait_for(crew.kickoff_async(), timeout=SUBTASK_TIMEOUT)

                end_time = time.time()

                # Extract token usage if available
                token_usage = TokenUsage()
                if result.token_usage:
                    token_usage = TokenUsage(
                        total_tokens=result.token_usage.total_tokens or 0,
                        prompt_tokens=result.token_usage.prompt_tokens or 0,
                        completion_tokens=result.token_usage.completion_tokens or 0,
                        cached_prompt_tokens=result.token_usage.cached_prompt_tokens or 0,
                    )

                # Create processing time tracking
                processing_time = ProcessingTime(
                    start_time=start_time, end_time=end_time, duration=end_time - start_time
                )

                # Create telemetry object
                telemetry = Telemetry(token_usage=token_usage, processing_time=processing_time)

                # Safely extract output string
                output_str = ""
                try:
                    output_str = str(result.raw)

                    # Check for iteration limits
                    if output_str and ("Maximum iterations reached" in output_str or "iteration limit" in output_str):
                        logger.warning(f"Task hit iteration limit: {subtask}")
                        output_str = truncate_text(output_str, 500)
                except Exception as e:
                    logger.error(f"Error extracting result for subtask '{subtask}': {e}")
                    output_str = "Task completed but output could not be extracted properly"

                # Return structured output
                return SubtaskOutput(
                    subtask=subtask,
                    output=output_str,
                    agents=agents,
                    telemetry=telemetry,
                )

            except TimeoutError:
                # Handle timeout
                end_time = time.time()
                processing_time = ProcessingTime(
                    start_time=start_time, end_time=end_time, duration=end_time - start_time
                )
                telemetry = Telemetry(processing_time=processing_time)

                logger.warning(f"Task timed out: {subtask}")
                return SubtaskOutput(
                    subtask=subtask,
                    output=f"Task timed out. {DEFAULT_TASK_OUTPUT}",
                    agents=agents,
                    telemetry=telemetry,
                )

            except Exception as e:
                # Handle any other exceptions
                end_time = time.time()
                processing_time = ProcessingTime(
                    start_time=start_time, end_time=end_time, duration=end_time - start_time
                )
                telemetry = Telemetry(processing_time=processing_time)

                logger.error(f"Error executing task '{subtask}': {str(e)}")
                return SubtaskOutput(
                    subtask=subtask,
                    output=f"Error: {str(e)[:100]}",
                    agents=agents,
                    telemetry=telemetry,
                )

        # Execute subtasks sequentially and accumulate results
        completed_outputs = []
        for assignment in self.state.assignments:
            # Emit dispatch event if streaming
            if self.request_id:
                await emit_subtask_dispatch(self.request_id, assignment.subtask, assignment.agents)

            output = await _execute(assignment.subtask, assignment.agents, completed_outputs)
            completed_outputs.append(output)

            # Emit result event if streaming with telemetry data
            if self.request_id:
                # Convert telemetry to dict format for JSON serialization
                telemetry_dict = None
                if output.telemetry:
                    telemetry_dict = {
                        "processing_time": (
                            {
                                "start_time": output.telemetry.processing_time.start_time,
                                "end_time": output.telemetry.processing_time.end_time,
                                "duration": output.telemetry.processing_time.duration,
                            }
                            if output.telemetry.processing_time
                            else None
                        ),
                        "token_usage": (
                            {
                                "total_tokens": output.telemetry.token_usage.total_tokens,
                                "prompt_tokens": output.telemetry.token_usage.prompt_tokens,
                                "completion_tokens": output.telemetry.token_usage.completion_tokens,
                                "cached_prompt_tokens": output.telemetry.token_usage.cached_prompt_tokens,
                            }
                            if output.telemetry.token_usage
                            else None
                        ),
                    }
                await emit_subtask_result(
                    self.request_id, assignment.subtask, output.output, assignment.agents, telemetry_dict
                )

        # Store outputs in state
        self.state.subtask_outputs = completed_outputs

    # 5ï¸âƒ£  Synthesise final answer ---------------------------------------
    @listen(run_sub_crews)
    async def synthesise(self) -> Dict[str, Any]:
        # Emit synthesis start event if streaming
        if self.request_id:
            await emit_synthesis_start(self.request_id)

        # Create comprehensive synthesis prompt
        prompt = (
            "Synthesize these results into a clear and thorough answer that directly addresses the user's request. "
            "Ensure your response is well-structured and covers all relevant information from the results. "
            "While being comprehensive, maintain clarity by organizing key points logically. "
            "Focus on providing actionable insights and complete explanations where needed. "
            "Do not include meta-commentary or phrases like 'based on the results'. "
            "Simply provide the synthesized answer in a natural, informative way.\n\n"
            f"User request: {self.state.chat_prompt}\n\n"
            "Results:\n"
        )

        # Optimize and include context from subtask outputs
        for i, subtask_output in enumerate(self.state.subtask_outputs):
            # Use context optimization to preserve important information
            optimized_output = optimize_context_block(
                subtask_output.output,
                max_length=15000 if i < 2 else 5000,  # Allow more content for first outputs
                preserve_start=500,  # Preserve more context from start
                preserve_end=300,  # Preserve key conclusions at end
            )

            # Include subtask details and agent information
            agent_info = f"[Executed by: {', '.join(subtask_output.agents)}]" if subtask_output.agents else ""
            prompt += f"\n{i+1}. Task: {subtask_output.subtask}\n{agent_info}\nOutput:\n{optimized_output}\n"

        # Always prioritize user-selected LLM (MOR) for final synthesis
        if self.user_selected_model:
            logger.info(f"ðŸ”¥ ORCHESTRATION - Using MOR LLM for final synthesis: {self.user_selected_model}")
            llm = self.llm_instance
        else:
            logger.info(f"âš ï¸ ORCHESTRATION - Falling back to Gemini for final synthesis: {self.standard_model}")
            llm = LLM(model=self.standard_model, api_key=self.standard_model_api_key)

        @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(Exception,))
        def synthesize_final_answer():
            return llm.call(prompt)

        try:
            resp = synthesize_final_answer()
            self.state.final_answer = resp.strip()

            # Identify and extract potential final answer actions
            self.state.final_answer_actions = await extract_final_answer_actions(
                chat_prompt=self.state.chat_prompt,
                final_answer=self.state.final_answer,
                standard_model=self.standard_model,
                efficient_model=self.efficient_model,
                standard_model_api_key=self.standard_model_api_key,
            )

        except Exception as e:
            logger.error(f"Failed to synthesize final answer: {e}")
            # Fallback: concatenate subtask outputs
            self.state.final_answer = "\n\n".join(
                f"{i+1}. {output.subtask}\n{output.output}" for i, output in enumerate(self.state.subtask_outputs)
            )
            self.state.final_answer_actions = []

        # Emit synthesis complete, final answer actions, and final complete events if streaming
        if self.request_id:
            await emit_synthesis_complete(self.request_id, self.state.final_answer, self.state.final_answer_actions)

            # Emit final answer actions if any were identified
            if self.state.final_answer_actions:
                await emit_final_answer_actions(self.request_id, self.state.final_answer_actions)

            await emit_flow_end(self.request_id)
            await emit_final_complete(self.request_id, self.state.final_answer_actions)

        return {
            "final_answer": self.state.final_answer,
            "subtask_outputs": self.state.subtask_outputs,
            "final_answer_actions": self.state.final_answer_actions,
        }
